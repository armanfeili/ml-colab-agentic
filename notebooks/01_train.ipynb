{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d6d87a",
   "metadata": {},
   "source": [
    "# Colab-First Minimal Trainer\n",
    "\n",
    "This notebook trains a simple CNN on CIFAR-10 in Google Colab with GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9e8d3",
   "metadata": {},
   "source": [
    "## Section A: Setup & Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal deps for run management & logging\n",
    "!pip -q install pyyaml pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36891346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, yaml, time, shutil, pandas as pd\n",
    "\n",
    "# --- Base paths (Colab) ---\n",
    "PROJECT = \"/content/ml-colab-agentic\"\n",
    "DATA_DIR = f\"{PROJECT}/data\"\n",
    "RUNS_DIR = f\"{PROJECT}/runs\"\n",
    "\n",
    "# Optional Drive mount toggle\n",
    "SAVE_TO_DRIVE = True\n",
    "DRIVE_DIR = \"/content/drive/MyDrive/ml-colab-agentic\"  # change if needed\n",
    "\n",
    "# --- Create a run folder ---\n",
    "def new_run_id(dataset, model, note=\"\"):\n",
    "    ts = time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    return \"_\".join(x for x in [ts, dataset, model, note] if x)\n",
    "\n",
    "RUN_ID  = new_run_id(\"cifar10\", \"simplenet\", \"amp\")\n",
    "RUN_DIR = f\"{RUNS_DIR}/{RUN_ID}\"\n",
    "for sub in [\n",
    "    \"checkpoints\",\n",
    "    \"plots/train\",\"plots/val\",\"plots/test\",\"plots/calib\",\n",
    "    \"artifacts/train\",\"artifacts/val\",\"artifacts/test\",\"artifacts/calib\",\n",
    "    \"cache\"\n",
    "]:\n",
    "    pathlib.Path(f\"{RUN_DIR}/{sub}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Write cfg.yaml (freeze your config; edit as needed before training) ---\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"data_root\": f\"{DATA_DIR}/raw\",   # downloads land here\n",
    "    \"num_workers\": 2,\n",
    "    \"amp\": True,\n",
    "}\n",
    "with open(f\"{RUN_DIR}/cfg.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(CFG, f)\n",
    "\n",
    "# --- Convenience symlinks to \"latest\" (optional) ---\n",
    "def safe_symlink(src, dst):\n",
    "    try:\n",
    "        if os.path.islink(dst) or os.path.exists(dst):\n",
    "            os.remove(dst)\n",
    "        os.symlink(src, dst)\n",
    "    except Exception:\n",
    "        # on Drive/Windows symlink may fail → just copy\n",
    "        if os.path.isdir(src):\n",
    "            if os.path.exists(dst): shutil.rmtree(dst)\n",
    "            shutil.copytree(src, dst)\n",
    "        else:\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "safe_symlink(RUN_DIR, f\"{PROJECT}/outputs\")                 # latest outputs → runs/<id>\n",
    "safe_symlink(f\"{RUN_DIR}/checkpoints\", f\"{PROJECT}/checkpoints\")\n",
    "print(\"RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-form metrics CSV: split, epoch, metric, value\n",
    "METRICS_CSV = f\"{RUN_DIR}/metrics.csv\"\n",
    "\n",
    "def log_metrics(rows):\n",
    "    \"\"\"\n",
    "    rows: list of dicts with keys: split, epoch, metric, value\n",
    "    example:\n",
    "      log_metrics([\n",
    "        {\"split\":\"train\",\"epoch\":1,\"metric\":\"loss\",\"value\":1.234},\n",
    "        {\"split\":\"val\",\"epoch\":1,\"metric\":\"acc\",\"value\":0.78},\n",
    "      ])\n",
    "    \"\"\"\n",
    "    import pandas as _pd, os as _os\n",
    "    df = _pd.DataFrame(rows, columns=[\"split\",\"epoch\",\"metric\",\"value\"])\n",
    "    df.to_csv(METRICS_CSV, mode=\"a\", header=not _os.path.exists(METRICS_CSV), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275deabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone/update the repo into /content/\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_PATH = '/content/ml-colab-agentic'\n",
    "REPO_URL = 'https://github.com/armanfeili/ml-colab-agentic.git'\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"{REPO_PATH} already exists. Updating...\")\n",
    "    subprocess.run(['git', '-C', REPO_PATH, 'pull'], check=True)\n",
    "else:\n",
    "    print(f\"Cloning {REPO_URL}...\")\n",
    "    subprocess.run(['git', 'clone', REPO_URL, REPO_PATH], check=True)\n",
    "\n",
    "print(f\"Repository ready at {REPO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e72bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (optional but recommended)\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "print(\"Google Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ccb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -q -r /content/ml-colab-agentic/requirements.txt\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba878304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo to path and verify imports\n",
    "import sys\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "\n",
    "from src.utils import (\n",
    "    set_seed,\n",
    "    get_device,\n",
    "    prepare_dataloaders_cifar10,\n",
    "    SimpleNet,\n",
    "    train_one_epoch,\n",
    "    evaluate,\n",
    "    save_checkpoint,\n",
    "    append_metrics_csv,\n",
    ")\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893679a9",
   "metadata": {},
   "source": [
    "## Section B: Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"num_workers\": 2,\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"data_root\": \"/content/data\",\n",
    "    \"save_to_drive\": True,\n",
    "    \"drive_dir\": \"/content/drive/MyDrive/ml-outputs\",\n",
    "}\n",
    "\n",
    "print(\"Config:\")\n",
    "for key, val in CFG.items():\n",
    "    print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb334de7",
   "metadata": {},
   "source": [
    "## Section C: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71446a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed and device\n",
    "set_seed(CFG[\"seed\"])\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataloaders\n",
    "print(f\"Loading {CFG['dataset']} from {CFG['data_root']}...\")\n",
    "train_dl, test_dl = prepare_dataloaders_cifar10(\n",
    "    root=CFG[\"data_root\"],\n",
    "    batch_size=CFG[\"batch_size\"],\n",
    "    num_workers=CFG[\"num_workers\"],\n",
    ")\n",
    "print(f\"✅ Train batches: {len(train_dl)}, Test batches: {len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = SimpleNet(num_classes=10).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Results will be saved to: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"\\nTraining for {CFG['epochs']} epochs...\\n\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(CFG[\"epochs\"]):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dl, opt, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc = evaluate(model, test_dl, device)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    epoch_ckpt = f\"{RUN_DIR}/checkpoints/epoch_{epoch+1:03d}.pt\"\n",
    "    save_checkpoint(model, epoch_ckpt)\n",
    "    \n",
    "    # Update best checkpoint if improved\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        import shutil\n",
    "        shutil.copy2(epoch_ckpt, f\"{RUN_DIR}/checkpoints/best.pt\")\n",
    "    \n",
    "    # Log metrics (long-form)\n",
    "    log_metrics([\n",
    "        {\"split\": \"train\", \"epoch\": epoch+1, \"metric\": \"loss\", \"value\": train_loss},\n",
    "        {\"split\": \"train\", \"epoch\": epoch+1, \"metric\": \"acc\", \"value\": train_acc},\n",
    "        {\"split\": \"val\", \"epoch\": epoch+1, \"metric\": \"loss\", \"value\": val_loss},\n",
    "        {\"split\": \"val\", \"epoch\": epoch+1, \"metric\": \"acc\", \"value\": val_acc},\n",
    "    ])\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{CFG['epochs']} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n✅ Training complete! Best val acc: {best_val_acc:.4f}\")\n",
    "print(f\"Results saved to: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ff8fd",
   "metadata": {},
   "source": [
    "## Section D: Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show run directory contents\n",
    "import os\n",
    "\n",
    "print(f\"Run directory: {RUN_DIR}\\n\")\n",
    "\n",
    "# List all files in the run directory\n",
    "for root, dirs, files in os.walk(RUN_DIR):\n",
    "    level = root.replace(RUN_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        fpath = os.path.join(root, file)\n",
    "        size = os.path.getsize(fpath) / (1024)  # KB\n",
    "        print(f'{subindent}{file} ({size:.1f} KB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36510d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy entire run to Google Drive (if enabled)\n",
    "if SAVE_TO_DRIVE:\n",
    "    import subprocess, shlex\n",
    "    \n",
    "    dst = f\"{DRIVE_DIR}/runs/{RUN_ID}\"\n",
    "    os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
    "    \n",
    "    # Use cp -r to copy entire run directory\n",
    "    result = subprocess.run(\n",
    "        shlex.split(f'cp -r \"{RUN_DIR}\" \"{dst}\"'),\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        check=False\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✅ Copied run to Drive: {dst}\")\n",
    "    else:\n",
    "        print(f\"⚠️  Copy to Drive failed: {result.stderr}\")\n",
    "else:\n",
    "    print(f\"📁 Run saved locally: {RUN_DIR}\")\n",
    "    print(f\"   (Set SAVE_TO_DRIVE=True to copy to Google Drive)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b815658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics table\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists(METRICS_CSV):\n",
    "    df = pd.read_csv(METRICS_CSV)\n",
    "    print(\"Training Metrics (long-form):\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Pivot for easier viewing\n",
    "    print(\"\\n\\nPivoted view:\")\n",
    "    pivot = df.pivot_table(index=['split', 'epoch'], columns='metric', values='value')\n",
    "    print(pivot)\n",
    "else:\n",
    "    print(\"No metrics file found.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
