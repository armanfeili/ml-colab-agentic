{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d6d87a",
   "metadata": {},
   "source": [
    "# Colab-First Minimal Trainer\n",
    "\n",
    "This notebook trains a simple CNN on CIFAR-10 in Google Colab with GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9e8d3",
   "metadata": {},
   "source": [
    "## Section A: Setup & Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "print(\"‚úÖ Google Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc211562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, time, yaml, shutil, pandas as pd\n",
    "\n",
    "# ---- Project anchors ----\n",
    "GDRIVE_ROOT = \"/content/drive/MyDrive\"\n",
    "PROJECT_NAME = \"ml-colab-agentic\"        # change if you fork/rename\n",
    "PROJECT_DRIVE = f\"{GDRIVE_ROOT}/{PROJECT_NAME}\"\n",
    "\n",
    "# Code lives in Colab VM from GitHub clone; data/runs live in Drive:\n",
    "DATA_DIR = f\"{PROJECT_DRIVE}/data\"       # data/raw, data/processed\n",
    "RUNS_DIR = f\"{PROJECT_DRIVE}/runs\"       # one folder per training run\n",
    "LATEST_DIR = f\"{PROJECT_DRIVE}/latest\"   # optional: points to latest run\n",
    "\n",
    "# Ensure minimal skeleton on Drive\n",
    "for p in [\n",
    "    f\"{DATA_DIR}/raw\", f\"{DATA_DIR}/processed\",\n",
    "    RUNS_DIR, f\"{LATEST_DIR}\"\n",
    "]:\n",
    "    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ DATA_DIR:\", DATA_DIR)\n",
    "print(\"üìÅ RUNS_DIR:\", RUNS_DIR)\n",
    "print(\"üìÅ LATEST_DIR:\", LATEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_run_id(dataset, model, note=\"\"):\n",
    "    ts = time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    return \"_\".join(x for x in [ts, dataset, model, note] if x)\n",
    "\n",
    "RUN_ID  = new_run_id(\"cifar10\", \"simplenet\", \"amp\")\n",
    "RUN_DIR = f\"{RUNS_DIR}/{RUN_ID}\"\n",
    "\n",
    "# Create per-run subfolders on Drive\n",
    "subs = [\n",
    "    \"checkpoints\",\n",
    "    \"plots/train\",\"plots/val\",\"plots/test\",\"plots/calib\",\n",
    "    \"artifacts/train\",\"artifacts/val\",\"artifacts/test\",\"artifacts/calib\",\n",
    "    \"cache\"\n",
    "]\n",
    "for s in subs:\n",
    "    pathlib.Path(f\"{RUN_DIR}/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the frozen config for reproducibility\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    \"data_root\": f\"{DATA_DIR}/raw\",\n",
    "    \"num_workers\": 2,\n",
    "    \"amp\": True,\n",
    "}\n",
    "with open(f\"{RUN_DIR}/cfg.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(CFG, f)\n",
    "\n",
    "# Convenience: point a \"latest\" folder to this run (copy if symlink fails)\n",
    "def safe_point_latest(src, dst):\n",
    "    try:\n",
    "        if os.path.islink(dst) or os.path.exists(dst):\n",
    "            if os.path.islink(dst):\n",
    "                os.unlink(dst)\n",
    "            else:\n",
    "                shutil.rmtree(dst)\n",
    "        os.symlink(src, dst)\n",
    "    except Exception:\n",
    "        shutil.copytree(src, dst)\n",
    "\n",
    "safe_point_latest(RUN_DIR, f\"{LATEST_DIR}/run\")\n",
    "print(\"üè∑Ô∏è RUN_ID:\", RUN_ID)\n",
    "print(\"üìÅ RUN_DIR:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_CSV = f\"{RUN_DIR}/metrics.csv\"\n",
    "\n",
    "def log_metrics(rows):\n",
    "    \"\"\"\n",
    "    rows: list of dicts with keys: split, epoch, metric, value\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(rows, columns=[\"split\",\"epoch\",\"metric\",\"value\"])\n",
    "    df.to_csv(METRICS_CSV, mode=\"a\", header=not os.path.exists(METRICS_CSV), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275deabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone/update the repo into /content/\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "REPO_PATH = '/content/ml-colab-agentic'\n",
    "REPO_URL = 'https://github.com/armanfeili/ml-colab-agentic.git'\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"{REPO_PATH} already exists. Updating...\")\n",
    "    subprocess.run(['git', '-C', REPO_PATH, 'pull'], check=True)\n",
    "else:\n",
    "    print(f\"Cloning {REPO_URL}...\")\n",
    "    subprocess.run(['git', 'clone', REPO_URL, REPO_PATH], check=True)\n",
    "\n",
    "print(f\"Repository ready at {REPO_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ccb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies from requirements.txt\n",
    "!pip install -q -r /content/ml-colab-agentic/requirements.txt\n",
    "print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba878304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo to path and verify imports\n",
    "import sys\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "\n",
    "from src.utils import (\n",
    "    set_seed,\n",
    "    get_device,\n",
    "    prepare_dataloaders_cifar10,\n",
    "    SimpleNet,\n",
    "    train_one_epoch,\n",
    "    evaluate,\n",
    "    save_checkpoint,\n",
    "    append_metrics_csv,\n",
    ")\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893679a9",
   "metadata": {},
   "source": [
    "## Section B: Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display current configuration (already saved to cfg.yaml)\n",
    "print(\"Current Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, val in CFG.items():\n",
    "    print(f\"  {key:20s}: {val}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n‚úÖ Config saved to: {RUN_DIR}/cfg.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb334de7",
   "metadata": {},
   "source": [
    "## Section C: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71446a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed and device\n",
    "set_seed(CFG[\"seed\"])\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataloaders\n",
    "print(f\"Loading {CFG['dataset']} from {CFG['data_root']}...\")\n",
    "train_dl, test_dl = prepare_dataloaders_cifar10(\n",
    "    root=CFG[\"data_root\"],\n",
    "    batch_size=CFG[\"batch_size\"],\n",
    "    num_workers=CFG[\"num_workers\"],\n",
    ")\n",
    "print(f\"‚úÖ Train batches: {len(train_dl)}, Test batches: {len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and optimizer\n",
    "model = SimpleNet(num_classes=10).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
    "\n",
    "print(f\"Model initialized on {device}\")\n",
    "print(f\"Results will be saved to: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"\\nTraining for {CFG['epochs']} epochs...\\n\")\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(CFG[\"epochs\"]):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dl, opt, device)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_acc = evaluate(model, test_dl, device)\n",
    "    \n",
    "    # Save checkpoint\n",
    "    epoch_ckpt = f\"{RUN_DIR}/checkpoints/epoch_{epoch+1:03d}.pt\"\n",
    "    save_checkpoint(model, epoch_ckpt)\n",
    "    \n",
    "    # Update best checkpoint if improved\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        import shutil\n",
    "        shutil.copy2(epoch_ckpt, f\"{RUN_DIR}/checkpoints/best.pt\")\n",
    "    \n",
    "    # Log metrics (long-form)\n",
    "    log_metrics([\n",
    "        {\"split\": \"train\", \"epoch\": epoch+1, \"metric\": \"loss\", \"value\": train_loss},\n",
    "        {\"split\": \"train\", \"epoch\": epoch+1, \"metric\": \"acc\", \"value\": train_acc},\n",
    "        {\"split\": \"val\", \"epoch\": epoch+1, \"metric\": \"loss\", \"value\": val_loss},\n",
    "        {\"split\": \"val\", \"epoch\": epoch+1, \"metric\": \"acc\", \"value\": val_acc},\n",
    "    ])\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{CFG['epochs']} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best val acc: {best_val_acc:.4f}\")\n",
    "print(f\"Results saved to: {RUN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ff8fd",
   "metadata": {},
   "source": [
    "## Section D: Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show run directory contents\n",
    "import os\n",
    "\n",
    "print(f\"Run directory: {RUN_DIR}\\n\")\n",
    "\n",
    "# List all files in the run directory\n",
    "for root, dirs, files in os.walk(RUN_DIR):\n",
    "    level = root.replace(RUN_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        fpath = os.path.join(root, file)\n",
    "        size = os.path.getsize(fpath) / (1024)  # KB\n",
    "        print(f'{subindent}{file} ({size:.1f} KB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36510d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All artifacts are already on Google Drive!\n",
    "print(\"‚úÖ All artifacts saved to Google Drive:\")\n",
    "print(f\"   {RUN_DIR}\")\n",
    "print(f\"\\nüìç Access from any device:\")\n",
    "print(f\"   Google Drive ‚Üí MyDrive ‚Üí {PROJECT_NAME} ‚Üí runs ‚Üí {RUN_ID}\")\n",
    "print(f\"\\n\udca1 Latest run always available at:\")\n",
    "print(f\"   {LATEST_DIR}/run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b815658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics table\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists(METRICS_CSV):\n",
    "    df = pd.read_csv(METRICS_CSV)\n",
    "    print(\"Training Metrics (long-form):\")\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Pivot for easier viewing\n",
    "    print(\"\\n\\nPivoted view:\")\n",
    "    pivot = df.pivot_table(index=['split', 'epoch'], columns='metric', values='value')\n",
    "    print(pivot)\n",
    "else:\n",
    "    print(\"No metrics file found.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
