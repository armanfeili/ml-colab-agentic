{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d6d87a",
   "metadata": {},
   "source": [
    "# Colab-First ML Training Pipeline\n",
    "\n",
    "This notebook trains a simple CNN on CIFAR-10 in Google Colab with GPU acceleration.\n",
    "\n",
    "**Architecture:**\n",
    "- **Code:** Stored in GitHub (`/content/ml-colab-agentic` in Colab VM) ‚Äî edit locally with Copilot, push to GitHub\n",
    "- **Data/Runs/Checkpoints:** Stored in Google Drive (`MyDrive/ml-colab-agentic/`) ‚Äî persists across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9e8d3",
   "metadata": {},
   "source": [
    "## Section A ‚Äî Setup (Drive + Repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A0) Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "print(\"‚úÖ Google Drive mounted at /content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc211562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1) Paths: code in Colab VM; data/runs in Drive\n",
    "import os, sys, pathlib, time, shutil, subprocess, yaml, pandas as pd\n",
    "\n",
    "# ---- Drive anchors ----\n",
    "GDRIVE_ROOT   = \"/content/drive/MyDrive\"\n",
    "PROJECT_NAME  = \"ml-colab-agentic\"   # change if you fork/rename\n",
    "PROJECT_DRIVE = f\"{GDRIVE_ROOT}/{PROJECT_NAME}\"\n",
    "\n",
    "# Storage in Drive\n",
    "DATA_DIR   = f\"{PROJECT_DRIVE}/data\"      # data/raw, data/processed\n",
    "RUNS_DIR   = f\"{PROJECT_DRIVE}/runs\"      # one folder per training run\n",
    "LATEST_DIR = f\"{PROJECT_DRIVE}/latest\"    # points to latest run\n",
    "\n",
    "# Ensure Drive skeleton\n",
    "for p in [f\"{DATA_DIR}/raw\", f\"{DATA_DIR}/processed\", RUNS_DIR, LATEST_DIR]:\n",
    "    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ DATA_DIR:\", DATA_DIR)\n",
    "print(\"üìÅ RUNS_DIR:\", RUNS_DIR)\n",
    "print(\"üìÅ LATEST_DIR:\", LATEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7339ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2) Clone/Update code from GitHub ‚Üí /content (code stays in the VM)\n",
    "REPO_OWNER = \"armanfeili\"                              # <‚Äî your GitHub username\n",
    "REPO_NAME  = \"ml-colab-agentic\"\n",
    "REPO_URL   = f\"https://github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n",
    "REPO_PATH  = f\"/content/{REPO_NAME}\"\n",
    "\n",
    "# Optional: for private repos set a PAT in GITHUB_TOKEN (Colab session env)\n",
    "token = os.environ.get(\"GITHUB_TOKEN\", \"\").strip()\n",
    "if token:\n",
    "    REPO_URL = f\"https://{token}:x-oauth-basic@github.com/{REPO_OWNER}/{REPO_NAME}.git\"\n",
    "\n",
    "if os.path.exists(REPO_PATH):\n",
    "    print(f\"{REPO_PATH} exists ‚Üí pulling latest...\")\n",
    "    subprocess.run([\"git\", \"-C\", REPO_PATH, \"fetch\", \"--prune\"], check=True)\n",
    "    subprocess.run([\"git\", \"-C\", REPO_PATH, \"checkout\", \"main\"], check=True)\n",
    "    subprocess.run([\"git\", \"-C\", REPO_PATH, \"pull\", \"--ff-only\"], check=True)\n",
    "else:\n",
    "    print(f\"Cloning {REPO_URL} ‚Üí {REPO_PATH} ...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"--depth=1\", REPO_URL, REPO_PATH], check=True)\n",
    "\n",
    "print(\"‚úÖ Repository ready at:\", REPO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be8cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A3) Install Python deps (from the repo's requirements.txt)\n",
    "subprocess.run([\"pip\", \"install\", \"-q\", \"-r\", f\"{REPO_PATH}/requirements.txt\"], check=True)\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275deabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A4) Add repo to import path and verify imports\n",
    "sys.path.insert(0, REPO_PATH)\n",
    "\n",
    "from src.utils import (\n",
    "    set_seed, get_device, prepare_dataloaders_cifar10, SimpleNet,\n",
    "    train_one_epoch, evaluate, save_checkpoint, append_metrics_csv\n",
    ")\n",
    "import torch, torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"‚úÖ Imports OK | torch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdae10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A5) (Optional) GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ccb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section B ‚Äî Run Config (frozen to Drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba878304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B0) Create a timestamped run folder in Drive and freeze config\n",
    "def new_run_id(dataset, model, note=\"\"):\n",
    "    ts = time.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    return \"_\".join(x for x in [ts, dataset, model, note] if x)\n",
    "\n",
    "RUN_ID  = new_run_id(\"cifar10\", \"simplenet\", \"amp\")\n",
    "RUN_DIR = f\"{RUNS_DIR}/{RUN_ID}\"\n",
    "\n",
    "subfolders = [\n",
    "    \"checkpoints\",\n",
    "    \"plots/train\",\"plots/val\",\"plots/test\",\"plots/calib\",\n",
    "    \"artifacts/train\",\"artifacts/val\",\"artifacts/test\",\"artifacts/calib\",\n",
    "    \"cache\",\n",
    "]\n",
    "for s in subfolders:\n",
    "    pathlib.Path(f\"{RUN_DIR}/{s}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 1e-3,\n",
    "    \"dataset\": \"CIFAR10\",\n",
    "    # IMPORTANT: dataset root is in Drive (cloud storage), not in /content\n",
    "    \"data_root\": f\"{DATA_DIR}/raw\",\n",
    "    \"num_workers\": 2,\n",
    "    \"amp\": True,\n",
    "}\n",
    "\n",
    "with open(f\"{RUN_DIR}/cfg.yaml\", \"w\") as f:\n",
    "    yaml.safe_dump(CFG, f)\n",
    "\n",
    "# Point \"latest\" ‚Üí this run (copy if symlink fails on Drive)\n",
    "def safe_point_latest(src, dst):\n",
    "    try:\n",
    "        if os.path.islink(dst) or os.path.exists(dst):\n",
    "            if os.path.islink(dst): os.unlink(dst)\n",
    "            else: shutil.rmtree(dst)\n",
    "        os.symlink(src, dst)\n",
    "    except Exception:\n",
    "        shutil.copytree(src, dst)\n",
    "\n",
    "safe_point_latest(RUN_DIR, f\"{LATEST_DIR}/run\")\n",
    "\n",
    "print(\"üè∑Ô∏è RUN_ID :\", RUN_ID)\n",
    "print(\"üìÅ RUN_DIR:\", RUN_DIR)\n",
    "print(\"‚úÖ Config frozen at:\", f\"{RUN_DIR}/cfg.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893679a9",
   "metadata": {},
   "source": [
    "# B1) Metrics logger (long-form CSV on Drive)\n",
    "METRICS_CSV = f\"{RUN_DIR}/metrics.csv\"\n",
    "\n",
    "def log_metrics(rows):\n",
    "    # rows = list[dict]: {split, epoch, metric, value}\n",
    "    df = pd.DataFrame(rows, columns=[\"split\",\"epoch\",\"metric\",\"value\"])\n",
    "    df.to_csv(METRICS_CSV, mode=\"a\", header=not os.path.exists(METRICS_CSV), index=False)\n",
    "\n",
    "print(\"‚úÖ Metrics will be appended to:\", METRICS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bfb9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section C ‚Äî Train (data & artifacts on Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb334de7",
   "metadata": {},
   "source": [
    "# C0) Seed & device\n",
    "set_seed(CFG[\"seed\"])\n",
    "device = get_device()\n",
    "print(f\"Device: {device} | CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71446a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1) DataLoaders ‚Äî downloads cached to Drive (CFG['data_root'])\n",
    "print(f\"Loading {CFG['dataset']} from {CFG['data_root']} ...\")\n",
    "train_dl, test_dl = prepare_dataloaders_cifar10(\n",
    "    root=CFG[\"data_root\"],\n",
    "    batch_size=CFG[\"batch_size\"],\n",
    "    num_workers=CFG[\"num_workers\"],\n",
    ")\n",
    "print(f\"‚úÖ Train batches: {len(train_dl)} | Test batches: {len(test_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4c40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2) Model + Optimizer\n",
    "model = SimpleNet(num_classes=10).to(device)\n",
    "opt   = optim.Adam(model.parameters(), lr=CFG[\"lr\"])\n",
    "print(\"‚úÖ Model initialized on\", device)\n",
    "print(\"üì¶ Outputs will be stored in:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3) Training loop ‚Üí saves checkpoints & metrics to Drive\n",
    "best_val_acc = 0.0\n",
    "E = CFG[\"epochs\"]\n",
    "\n",
    "print(f\"\\nüöÄ Training for {E} epochs ...\\n\")\n",
    "for epoch in range(1, E+1):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dl, opt, device)\n",
    "    val_loss,   val_acc   = evaluate(model, test_dl, device)\n",
    "\n",
    "    # Save epoch checkpoint in Drive\n",
    "    ckpt_path = f\"{RUN_DIR}/checkpoints/epoch_{epoch:03d}.pt\"\n",
    "    save_checkpoint(model, ckpt_path)\n",
    "\n",
    "    # Update best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        shutil.copy2(ckpt_path, f\"{RUN_DIR}/checkpoints/best.pt\")\n",
    "\n",
    "    # Log metrics to Drive\n",
    "    log_metrics([\n",
    "        {\"split\":\"train\",\"epoch\":epoch,\"metric\":\"loss\",\"value\":train_loss},\n",
    "        {\"split\":\"train\",\"epoch\":epoch,\"metric\":\"acc\",\"value\":train_acc},\n",
    "        {\"split\":\"val\",\"epoch\":epoch,\"metric\":\"loss\",\"value\":val_loss},\n",
    "        {\"split\":\"val\",\"epoch\":epoch,\"metric\":\"acc\",\"value\":val_acc},\n",
    "    ])\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d}/{E} | \"\n",
    "        f\"Train: loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
    "        f\"Val: loss={val_loss:.4f} acc={val_acc:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Training complete! Best val acc: {best_val_acc:.4f}\")\n",
    "print(\"üìÅ Run saved at:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Section D ‚Äî Inspect (everything is already on Drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ff8fd",
   "metadata": {},
   "source": [
    "# D0) Pretty-print run contents (Drive)\n",
    "for root, dirs, files in os.walk(RUN_DIR):\n",
    "    level = root.replace(RUN_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        fpath = os.path.join(root, file)\n",
    "        size_kb = os.path.getsize(fpath) / 1024\n",
    "        print(f\"{subindent}{file} ({size_kb:.1f} KB)\")\n",
    "\n",
    "print(\"\\n‚úÖ All artifacts are in Google Drive:\")\n",
    "print(f\"   MyDrive ‚Üí {PROJECT_NAME} ‚Üí runs ‚Üí {RUN_ID}\")\n",
    "print(\"üîó Latest run pointer:\", f\"{LATEST_DIR}/run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1) Show metrics (Drive CSV)\n",
    "import pandas as pd\n",
    "\n",
    "if os.path.exists(METRICS_CSV):\n",
    "    df = pd.read_csv(METRICS_CSV)\n",
    "    print(\"\\nTraining Metrics (long-form):\")\n",
    "    display(df)\n",
    "\n",
    "    print(\"\\nPivoted view:\")\n",
    "    pivot = df.pivot_table(index=['split','epoch'], columns='metric', values='value')\n",
    "    display(pivot)\n",
    "else:\n",
    "    print(\"No metrics file found at:\", METRICS_CSV)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
